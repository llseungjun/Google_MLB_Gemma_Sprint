{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 코드 실행 전 주의 사항\n",
        "\n",
        ">### **CPT 학습 후 허깅페이스에 업로드 시** 아래를 따라주세요\n",
        "**2.gemma 모델 로드 > 필요패키지 및 허깅페이스 세팅까지 실행 후 바로 3.Continued pre-training을 실행해주세요.**"
      ],
      "metadata": {
        "id": "kuiMOPT_7EBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.데이터 전처리\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "skAI3vPA3EZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# JSON 파일 불러오기\n",
        "with open('finetune_raw_dataset.json', 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "# JSON을 데이터프레임으로 변환\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "_iSQcnPpIWa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Wr4ZKXrwI-V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1.질문과 답변으로 나누기(이외 다른 컬럼들도 split)"
      ],
      "metadata": {
        "id": "ChrONDs33Mkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 'title' 컬럼의 각 문자열을 '/' 기준으로 분리\n",
        "def split_title(x):\n",
        "    parts = x.split('/')\n",
        "    # 반환할 때, 항상 3개의 값이 있도록 함\n",
        "    # 이렇게 하는 이유는 3개의 값이 없는 row의 경우엔 에러가 나오기 때문\n",
        "    return (parts + [None] * 3)[:3]\n",
        "\n",
        "# 'personality' split\n",
        "def split_person(x):\n",
        "    parts = x.split('/')\n",
        "    # 반환할 때, 항상 6개의 값이 있도록 함\n",
        "    return (parts + [None] * 6)[:6]\n",
        "\n",
        "# 'script' split\n",
        "def split_script(x):\n",
        "   # 숫자와 마침표를 기준으로 문자열을 분리하는 함수\n",
        "    parts = re.split(r'\\.\\n+\\d+\\.|\\d+\\-\\d|\\n+\\d+\\.', x)\n",
        "    # 빈 문자열 제거\n",
        "    return ([part for part in parts if part] + [None] * 5)[:5]\n",
        "\n",
        "def split_qna(x):\n",
        "    # 없는 문항이라면 바로 None을 반환\n",
        "    if x is None:\n",
        "        return [None]*2\n",
        "    # 기준 문자열을 포함하여 문자열 분리\n",
        "    parts = re.split(r'(자\\)|오\\.|바랍니다\\.|시오|세요)', x)\n",
        "    # 기준 문자열을 추가하여 결과 리스트 생성\n",
        "    result = []\n",
        "    for i in range(0, len(parts), 2):\n",
        "        if i+1 < len(parts):\n",
        "            result.append(parts[i] + parts[i+1])\n",
        "        else:\n",
        "            result.append(parts[i])\n",
        "    return (result + [None] * 2)[:2]\n",
        "\n",
        "df[['company', 'position', 'season']] = df['title'].apply(lambda x: pd.Series(split_title(x)))\n",
        "df[['univ', 'major', 'gpa', 'language', 'experience', 'qualifications']] = df['personality'].apply(lambda x: pd.Series(split_person(x)))\n",
        "df[['q1', 'q2', 'q3','q4','q5']] = df['script'].apply(lambda x: pd.Series(split_script(x)))\n",
        "\n",
        "qna_dataset = pd.DataFrame(columns=['q', 'a'])\n",
        "df_tmp = pd.DataFrame(columns=['q', 'a'])\n",
        "for i in range(1,6):\n",
        "    df_tmp[['q','a']] = df[f'q{i}'].apply(lambda x: pd.Series(split_qna(x)))\n",
        "    qna_dataset = pd.concat([qna_dataset,df_tmp])\n",
        "\n",
        "qna_dataest_cleaned = qna_dataset.dropna()\n",
        "qna_dataest_cleaned['a'] = qna_dataest_cleaned['a'].apply(lambda x: pd.Series(re.sub(r'\\n', ' ', x))) #답변의 개행문자 제거\n",
        "qna_dataest_cleaned['a'] = qna_dataest_cleaned['a'].apply(lambda x: pd.Series(re.sub(r'^\\.', ' ', x))) #답변의 시작 부분 온점 제거\n",
        "qna_dataest_cleaned = qna_dataest_cleaned.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "G1-npkW7I-TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. 이상치 제거(300자 이하인 답변)"
      ],
      "metadata": {
        "id": "mbi6iPpq3tlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qna_dataest_cleaned = qna_dataest_cleaned[qna_dataest_cleaned['a'].apply(lambda x: len(x)) > 300] # 답변의 길이가 300자 이상인 답변만 필터링"
      ],
      "metadata": {
        "id": "jWpJVR0EeBEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"filltering된 qa 개수 : \", len(qna_dataest_cleaned))"
      ],
      "metadata": {
        "id": "i05WgoFW5E24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-3.특수문자 제거"
      ],
      "metadata": {
        "id": "HlFS2u3D31X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qna_dataest_cleaned['a'] = qna_dataest_cleaned['a'].apply(lambda x: pd.Series(re.sub(r'^\\(.*?\\)|^\\s+\\(.*?\\)|^\\s+', '', x))) # 답변의 문장 시작 부분의 괄호(질문으로부터 잘려온 부분) 제거"
      ],
      "metadata": {
        "id": "Sp4EPBSCEWzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_dataest_cleaned.head()"
      ],
      "metadata": {
        "id": "xqqWMDMqEWxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_dataest_cleaned.to_csv('finetune_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "vu2y8U-wEWul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qna_dataest_cleaned.loc[3]['a'])"
      ],
      "metadata": {
        "id": "ZNy_p5DlEWsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.gemma 모델 로드"
      ],
      "metadata": {
        "id": "XUprEWmt2xzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요패키지 및 허깅페이스 세팅"
      ],
      "metadata": {
        "id": "111Rg67g98Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n",
        "!pip install datasets\n",
        "!pip install trl\n",
        "!pip install triton\n",
        "!pip uninstall xformers\n",
        "!pip install xformers"
      ],
      "metadata": {
        "id": "qghIuj4zmTUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "iJyTZzR54T3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as nu\n",
        "\n",
        "df = pd.read_csv('./finetune_dataset_ALL.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "JFIAN5D3MU_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# YOUR_HUGGINGFACE_TOKEN\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ZotG1QXoU-0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU 메모리 캐시 비우기\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 나중에 GPU 메모리 사용량을 확인\n",
        "print(torch.cuda.memory_allocated())\n",
        "print(torch.cuda.memory_reserved())"
      ],
      "metadata": {
        "id": "C9On3qH7WYT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기본 버전"
      ],
      "metadata": {
        "id": "ITBcLEGd9GT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### finetuning 가능한 형태로 데이터 전처리"
      ],
      "metadata": {
        "id": "LN8rGadeA6ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "# prompt 템플릿 적용\n",
        "def generate_prompt(example):\n",
        "    prompt_list = []\n",
        "    for i in range(len(example['q'])):\n",
        "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
        "다음 질문에 적절한 답변을 생성해주세요:\n",
        "\n",
        "{}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{}<end_of_turn><eos>\"\"\".format(example['q'][i], example['a'][i]))\n",
        "\n",
        "    return prompt_list\n",
        "\n",
        "df['prompt'] = generate_prompt(df)\n",
        "\n",
        "data = Dataset.from_pandas(df)\n",
        "\n",
        "# 학습할 데이터 토큰화\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "\n",
        "# train, test 분리\n",
        "data = data.train_test_split(test_size=0.2)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "r18N6HBYA63v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForQuestionAnswering, pipeline, BitsAndBytesConfig\n",
        "## base finetuning approach\n",
        "BASE_MODEL = \"rtzr/ko-gemma-2-9b-it\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "# 토크나이저는 양자화 필요없다! 꼭 모델 양자화 했는지 확인!!\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0})\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "# # pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "T1arLlNeQ9Or",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eI3_QPWY5sEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQUwUc6LACDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기본 버전 모델 학습"
      ],
      "metadata": {
        "id": "L5VLpXi-_m4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "\n",
        "# GPU 메모리 캐시 비우기\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 나중에 GPU 메모리 사용량을 확인\n",
        "print(torch.cuda.memory_allocated())\n",
        "print(torch.cuda.memory_reserved())\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "def formatting_func(example):\n",
        "    return example['prompt']\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    dataset_text_field = 'prompt',\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        # push_to_hub=True,\n",
        "        # push_to_hub_model_id=\"ko-QA-gemma-7b-finetuned\",\n",
        "        # push_to_hub_token=\"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\", # 허깅페이스에 push하는 코드\n",
        "        save_strategy=\"epoch\",\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "    ),\n",
        "    peft_config=lora_config,\n",
        "    formatting_func=formatting_func,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "YkKabVBZ_X81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 성능 체크\n",
        "def get_completion(query: str, model, tokenizer):\n",
        "\n",
        "  prompt_template = \"\"\"user\n",
        "  {query}\n",
        "\n",
        "  model\n",
        "  \"\"\"\n",
        "  prompt = prompt_template.format(query=query)\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "  model_inputs = encodeds.to(\"cuda:0\")\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=256)\n",
        "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "  return decoded"
      ],
      "metadata": {
        "id": "rJwFP6fk_X6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(query=data['test']['q'][0],\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Zv5qZay8_X3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(query=data['test']['q'][1],\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "LfO8wauauZZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']['a'][0]"
      ],
      "metadata": {
        "id": "HsLL6BA2jqvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "madYUe9eyJLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unsloth 버전"
      ],
      "metadata": {
        "id": "H8RIFvM49Sjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "   model_name = \"unsloth/gemma-2-9b\",\n",
        "   max_seq_length = max_seq_length,\n",
        "   dtype = dtype,\n",
        "   load_in_4bit = load_in_4bit)"
      ],
      "metadata": {
        "id": "kDD2AxGM9UCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "Q0mymuib9a9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### finetuning 가능한 형태로 데이터 전처리"
      ],
      "metadata": {
        "id": "3EqUgSzeBJd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "# prompt 템플릿 적용\n",
        "def generate_prompt(example):\n",
        "    prompt_list = []\n",
        "    for i in range(len(example['q'])):\n",
        "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
        "다음 질문에 적절한 답변을 생성해주세요:\n",
        "\n",
        "{}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{}<end_of_turn><eos>\"\"\".format(example['q'][i], example['a'][i]))\n",
        "\n",
        "    return prompt_list\n",
        "\n",
        "df['prompt'] = generate_prompt(df)\n",
        "\n",
        "data = Dataset.from_pandas(df)\n",
        "\n",
        "# 학습할 데이터 토큰화\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "\n",
        "# train, test 분리\n",
        "data = data.train_test_split(test_size=0.2)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "yAAfVSmOBHJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### unsloth 버전 모델 학습"
      ],
      "metadata": {
        "id": "uQgdmnnEBN7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = data[\"train\"],\n",
        "    dataset_text_field = 'prompt',\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "3OJ3kR_B9dL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ze7y5LBg9kME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "SPG5fUU59ncG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "vlnMz1s3EI5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 성능 체크"
      ],
      "metadata": {
        "id": "ckwyvDF2Ec4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 성능 체크\n",
        "def get_completion(query: str, model, tokenizer):\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "당신은 자기소개서를 작성하는 전문가입니다.\n",
        "자기소개서 항목 중 지원 동기를 특히 잘 만들어냅니다.\n",
        "다음 정보를 바탕으로 지원 동기를 작성해 주세요.\n",
        "\n",
        "지원 직무: AI 연구원\n",
        "지원 회사: 삼성전자\n",
        "개인 경험/기술: 머신러닝 프로젝트 경험, 소프트웨어 엔지니어 경력\n",
        "지원 동기:\"\"\"\n",
        "\n",
        "    prompt = prompt_template #prompt_template.format(query=)\n",
        "    # alpaca_prompt = Copied from above\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "    decoded = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "aVUx-hiwEbUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(query=data['test']['q'][2],\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "3WW7aKUKEbUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"당신이 누구인지 설명드리겠습니다.\n",
        "당신은 자기소개서를 작성하는 전문가입니다.\n",
        "자기소개서 항목 중 지원 동기를 특히 잘 만들어냅니다.\n",
        "다음 정보를 바탕으로 지원 동기를 작성해 주세요.\n",
        "\n",
        "지원 직무: AI 연구원\n",
        "지원 회사: 삼성전자\n",
        "개인 경험/기술: 머신러닝 프로젝트 경험, 소프트웨어 엔지니어 경력\n",
        "지원 동기:\"\"\"\n",
        "prompt = prompt_template #prompt_template.format(query=)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")"
      ],
      "metadata": {
        "id": "iN0zBXZ6KNeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "id": "kNzs5OHjTSqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['input_ids'].size()"
      ],
      "metadata": {
        "id": "SWsCnM6STXNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Continued pre-training"
      ],
      "metadata": {
        "id": "ZlpqQ8apdBA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-9b\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "vLab1qD6daaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "\n",
        "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,   # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "ZL429A7bd-8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 가능한 파라미터 수\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "jLcYxDZNeFH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### finetuning 가능한 형태로 데이터 전처리 + 메타데이터 포함"
      ],
      "metadata": {
        "id": "UhegsuzhfJ_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "# prompt 템플릿 적용\n",
        "def generate_prompt(example):\n",
        "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
        "\n",
        "    :param data_point: dict: Data point\n",
        "    :return: dict: tokenzed prompt\n",
        "    \"\"\"\n",
        "    prompt_list = []\n",
        "    for i in range(len(example['q'])):\n",
        "        prefix_text = '아래는 작업을 설명하는 지시사항입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\\\n\\\\n'\n",
        "        # 추가 맥락이 포함된 샘플.\n",
        "        text = (\n",
        "            f\"<start_of_turn>user {prefix_text} 지시사항: {example['q'][i]} \\\\n\\\\n\"\n",
        "            f\"입력 내용은 다음과 같습니다: 지원직무: {example['position'][i]}, \"\n",
        "            f\"지원회사: {example['company'][i]}, 개인경험: {example['personality'][i]} \"\n",
        "            f\"<end_of_turn>\\\\n<start_of_turn>model {example['a'][i]} <end_of_turn><eos>\"\n",
        "        )\n",
        "        prompt_list.append(text)\n",
        "    return prompt_list\n",
        "\n",
        "df['prompt'] = generate_prompt(df)\n",
        "\n",
        "data = Dataset.from_pandas(df)\n",
        "\n",
        "# 학습할 데이터 토큰화\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "\n",
        "# train, test 분리\n",
        "data = data.train_test_split(test_size=0.2)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "dcp3fc80efOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']['prompt'][1]"
      ],
      "metadata": {
        "id": "eSroor8yNzxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPT 모델 학습"
      ],
      "metadata": {
        "id": "10L0iChnfi-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = data[\"train\"],\n",
        "    dataset_text_field = 'prompt',\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 1,\n",
        "\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "YSxSYU01eyUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "S3riyLpNHNy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 성능 체크"
      ],
      "metadata": {
        "id": "QKiNBk9kfqLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "# 성능 체크\n",
        "def get_completion(example, model, tokenizer, i):\n",
        "    prompt = f\"\"\"다음 정보를 바탕으로 지원 동기를 작성해 주세요.\n",
        "\n",
        "    지원 직무: {example['position'][i]}\n",
        "    지원 회사: {example['company'][i]}\n",
        "    개인 경험/기술: {example['personality'][i]}\n",
        "\n",
        "    지원 동기:\"\"\"\n",
        "\n",
        "    # FastLanguageModel로 추론 성능 최적화\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    outputs = outputs = model.generate(\n",
        "        **inputs,\n",
        "        streamer=text_streamer,\n",
        "        temperature=0.7,       # Lower temperature for less randomness\n",
        "        top_p=0.9,             # Nucleus sampling (top-p)\n",
        "        top_k=50,              # Limits vocabulary size for each step\n",
        "        max_new_tokens=500,    # Limits the output length to avoid repetitions\n",
        "        repetition_penalty=1.2, # Penalizes repeated sequences\n",
        "        use_cache=True\n",
        "    )\n",
        "    decoded = tokenizer.batch_decode(outputs)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "ErkPgOkaeyRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(example=data['test'],\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        i = 1)\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "c8asmHE7fuM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "FNdmIB2JL_EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 저장"
      ],
      "metadata": {
        "id": "IAQ7DshiOYJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GGUF\n",
        "- Ollama로 serving 하기 위함\n",
        "- 허깅페이스에 업로드"
      ],
      "metadata": {
        "id": "z9h2ckFiTUtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ],
      "metadata": {
        "id": "uM4vRV-cUPCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPT로 학습된 모델 저장\n",
        "model=trainer.model\n",
        "\n",
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if True: model.push_to_hub_gguf(\"Goranii/gemma-2-9b-CPT-GGUF\", tokenizer, quantization_method = \"q4_k_m\", token = \"YOUR_HUGGINGFACE_TOKEN\") ## 4bit 모델 저장\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
      ],
      "metadata": {
        "id": "zL6hxv8AOX2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4bit model\n",
        "- vLLM으로 serving 하기 위함\n",
        "- 허깅페이스에 업로드"
      ],
      "metadata": {
        "id": "9J0mMQf_TguR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ],
      "metadata": {
        "id": "ABXJXKXxT3Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPT로 학습된 모델 저장\n",
        "model=trainer.model\n",
        "\n",
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if True: model.push_to_hub_merged(\"Goranii/gemma-2-9b-CPT\", tokenizer, save_method = \"merged_16bit\", token = \"YOUR_HUGGINGFACE_TOKEN\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"Goranii/gemma-2-9b-CPT\", tokenizer, save_method = \"merged_4bit_forced\", token = \"YOUR_HUGGINGFACE_TOKEN\") ## 4bit 모델 저장\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ],
      "metadata": {
        "id": "7grOJ9EHTytm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.메타데이터 포함 fine tuning"
      ],
      "metadata": {
        "id": "b1siYl66td0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 메타데이터를 포함한 데이터 import"
      ],
      "metadata": {
        "id": "he4BZmPptmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as nu\n",
        "\n",
        "df = pd.read_csv('./finetune_dataset_ALL.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gcvKaKqRthsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unsloth 버전"
      ],
      "metadata": {
        "id": "6FgKOagFuh3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "   model_name = \"unsloth/gemma-2-9b\",\n",
        "   max_seq_length = max_seq_length,\n",
        "   dtype = dtype,\n",
        "   load_in_4bit = load_in_4bit)"
      ],
      "metadata": {
        "id": "9I8SvjUhuh3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "TcRE1V8Kuh3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### finetuning 가능한 형태로 데이터 전처리"
      ],
      "metadata": {
        "id": "sM00clFjuh3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "# prompt 템플릿 적용\n",
        "def generate_prompt(example):\n",
        "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
        "\n",
        "    :param data_point: dict: Data point\n",
        "    :return: dict: tokenzed prompt\n",
        "    \"\"\"\n",
        "    prompt_list = []\n",
        "    for i in range(len(example['q'])):\n",
        "        prefix_text = '아래는 작업을 설명하는 지시사항입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\\\n\\\\n'\n",
        "        # 추가 맥락이 포함된 샘플.\n",
        "        text = (\n",
        "            f\"<start_of_turn>user {prefix_text} 지시사항: {example['q'][i]} \\\\n\\\\n\"\n",
        "            f\"입력 내용은 다음과 같습니다: 지원직무: {example['position'][i]}, \"\n",
        "            f\"지원회사: {example['company'][i]}, 개인경험: {example['personality'][i]} \"\n",
        "            f\"<end_of_turn>\\\\n<start_of_turn>model {example['a'][i]} <end_of_turn><eos>\"\n",
        "        )\n",
        "        prompt_list.append(text)\n",
        "    return prompt_list\n",
        "\n",
        "df['prompt'] = generate_prompt(df)\n",
        "\n",
        "data = Dataset.from_pandas(df)\n",
        "\n",
        "# 학습할 데이터 토큰화\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "\n",
        "# train, test 분리\n",
        "data = data.train_test_split(test_size=0.2)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "JhP4zRJsuh3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']['prompt'][1]"
      ],
      "metadata": {
        "id": "m2iC3X0ElZ3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### unsloth 버전 모델 학습"
      ],
      "metadata": {
        "id": "To6HLUcjuh3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = data[\"train\"],\n",
        "    dataset_text_field = 'prompt',\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "DZZUCxO3uh3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ccLpfkYiuh3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "PfvOQwssuh3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "SQ9LkHiZuh3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 성능 체크"
      ],
      "metadata": {
        "id": "YgAab1x9uh3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "# 성능 체크\n",
        "def get_completion(example, model, tokenizer, i):\n",
        "    prompt = f\"\"\"다음 정보를 바탕으로 지원 동기를 작성해 주세요.\n",
        "\n",
        "    지원 직무: {example['position'][i]}\n",
        "    지원 회사: {example['company'][i]}\n",
        "    개인 경험/기술: {example['personality'][i]}\n",
        "\n",
        "    지원 동기:\"\"\"\n",
        "\n",
        "    # FastLanguageModel로 추론 성능 최적화\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    outputs = outputs = model.generate(\n",
        "        **inputs,\n",
        "        streamer=text_streamer,\n",
        "        temperature=0.7,       # Lower temperature for less randomness\n",
        "        top_p=0.9,             # Nucleus sampling (top-p)\n",
        "        top_k=50,              # Limits vocabulary size for each step\n",
        "        max_new_tokens=200,    # Limits the output length to avoid repetitions\n",
        "        repetition_penalty=1.2, # Penalizes repeated sequences\n",
        "        use_cache=True\n",
        "    )\n",
        "    decoded = tokenizer.batch_decode(outputs)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "ZphwxNpTuh3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(example=data['test'],\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        i = 0)\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "B5fsIuFZuh3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "4fue_L1yqVRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "# 성능 체크\n",
        "def get_completion(example, model, tokenizer,i):\n",
        "\n",
        "    prompt_template = \"\"\"당신은 지원 동기 작성 어시스턴트입니다. 다음 정보를 바탕으로 지원 동기를 작성해 주세요.\n",
        "\n",
        "지원 직무: AI 연구원\n",
        "지원 회사: 삼성전자\n",
        "개인 경험/기술: 머신러닝 프로젝트 경험, 소프트웨어 엔지니어 경력\n",
        "\n",
        "지원 동기:\"\"\"#.format(data['test']['position'][i],data['test']['company'][i],data['test']['personality'][i],data['test']['q'][i])\n",
        "    prompt = prompt_template #prompt_template.format(query=)\n",
        "    # alpaca_prompt = Copied from above\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    outputs = outputs = model.generate(\n",
        "        **inputs,\n",
        "        streamer=text_streamer,\n",
        "        temperature=0.7,       # Lower temperature for less randomness\n",
        "        top_p=0.9,             # Nucleus sampling (top-p)\n",
        "        top_k=50,              # Limits vocabulary size for each step\n",
        "        max_new_tokens=200,    # Limits the output length to avoid repetitions\n",
        "        repetition_penalty=1.2, # Penalizes repeated sequences\n",
        "        use_cache=True\n",
        "    )\n",
        "    decoded = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "GtChjQwNeCaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(example=data['test'],\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        i = 0)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "m4LNptCdcXJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# 성능 체크\n",
        "def get_completion(example, model, tokenizer, i):\n",
        "    prompt_template = \"\"\"당신은 지원 동기 작성 어시스턴트입니다. 다음 정보를 바탕으로 지원 동기를 작성해 주세요.\n",
        "\n",
        "지원 직무: AI 연구원\n",
        "지원 회사: 삼성전자\n",
        "개인 경험/기술: 머신러닝 프로젝트 경험, 소프트웨어 엔지니어 경력\n",
        "\n",
        "지원 동기:\"\"\"\n",
        "    prompt = prompt_template\n",
        "\n",
        "    # Inference setup\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
        "\n",
        "    # Streaming output setup\n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    # Model generation with additional sampling parameters\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        streamer=text_streamer,\n",
        "        temperature=0.7,       # Lower temperature for less randomness\n",
        "        top_p=0.9,             # Nucleus sampling (top-p)\n",
        "        top_k=50,              # Limits vocabulary size for each step\n",
        "        max_new_tokens=200,    # Limits the output length to avoid repetitions\n",
        "        repetition_penalty=1.2, # Penalizes repeated sequences\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    # Decoding the generated tokens\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return decoded\n",
        "\n",
        "# 모델 실행 및 출력\n",
        "result = get_completion(\n",
        "    example=data['test'],\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    i=0\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "OGI_or1NfKga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]"
      ],
      "metadata": {
        "id": "KjP1s_T9YUHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5FzubAXphHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}